{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1、用于测试模型的Params\\GFLOPs\\FPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch\n",
    "import copy\n",
    "from thop import profile\n",
    "from thop import clever_format\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from torchstat import stat\n",
    "\n",
    "def show_macs_params(model, img_size=(256, 256), dummy_input=None):\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    if dummy_input is None:\n",
    "        dummy_input = torch.randn(1, 3, img_size[0], img_size[1], dtype=torch.float)\n",
    "    dummy_input = dummy_input.to(device)\n",
    "    \n",
    "     # macs == FLOPS, GFLOPS == 1e12 * FLOPS\n",
    "    macs, params = profile(model, inputs=(dummy_input,), verbose=False) \n",
    "    print(f\"{model._get_name()}\\t\\t{macs=}\\t{params=}\")\n",
    "    print(\"FLOPs=\", str(macs/1e9) +'{}'.format(\"G\"), end='\\t')\n",
    "    print(\"params=\", str(params/1e6)+'{}'.format(\"M\"))\n",
    "    macs, params = clever_format([macs, params], \"%.3f\")\n",
    "    print(f\"{macs=}\\t{params=}\")\n",
    "\n",
    "\n",
    "def inference_speed(model, img_size=(256, 256), dummy_input=None):\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    if dummy_input is None:\n",
    "        dummy_input = torch.randn(1, 3, img_size[0], img_size[1], dtype=torch.float)\n",
    "    dummy_input = dummy_input.to(device)\n",
    "\n",
    "    starter = torch.cuda.Event(enable_timing=True)\n",
    "    ender = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "    repetitions = 300\n",
    "    timings=np.zeros((repetitions,1))\n",
    "    #GPU-WARM-UP\n",
    "    for _ in range(10):\n",
    "        _ = model(dummy_input)\n",
    "    # MEASURE PERFORMANCE\n",
    "    with torch.no_grad():\n",
    "        for rep in range(repetitions):\n",
    "            starter.record()\n",
    "            _ = model(dummy_input)\n",
    "            ender.record()\n",
    "            # WAIT FOR GPU SYNC\n",
    "            torch.cuda.synchronize()\n",
    "            curr_time = starter.elapsed_time(ender)\n",
    "            timings[rep] = curr_time\n",
    "    mean_syn = np.sum(timings) / repetitions\n",
    "    std_syn = np.std(timings)\n",
    "    mean_fps = 1000. / mean_syn\n",
    "    print('Mean@ {mean_syn:.3f}ms Std@ {std_syn:.3f}ms FPS@ {mean_fps:.2f}'\\\n",
    "        .format(mean_syn=mean_syn, std_syn=std_syn, mean_fps=mean_fps))\n",
    "    # ! @n 中的n是什么意思？\n",
    "    # print(' * Mean@1 {mean_syn:.3f}ms Std@5 {std_syn:.3f}ms FPS@1 {mean_fps:.2f}'\\\n",
    "    #     .format(mean_syn=mean_syn, std_syn=std_syn, mean_fps=mean_fps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import constant_init, normal_init\n",
    "from models.pose_estimation.liteHandNet.common import channel_shuffle, ChannelAttension, SEBlock\n",
    "from models.pose_estimation.liteHandNet.repblocks import RepConv, RepBlock\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "\n",
    "class MSRB(nn.Module):\n",
    "    def __init__(self, channels, ca_type='none'):\n",
    "        super().__init__()\n",
    "        self.half_channels = channels // 2\n",
    "        self.branch1 = nn.ModuleList([\n",
    "            RepConv(self.half_channels, self.half_channels, 3, 1, 1,\n",
    "                    groups=self.half_channels, activation=None),\n",
    "            RepConv(self.half_channels, self.half_channels, 3, 1, 1,\n",
    "                    groups=self.half_channels, activation=None)\n",
    "             ])\n",
    "        self.branch2 = nn.ModuleList([\n",
    "            RepConv(self.half_channels, self.half_channels, 3, 1, 2, 2,\n",
    "                    groups=self.half_channels, activation=None),\n",
    "            RepConv(self.half_channels, self.half_channels, 3, 1, 2, 2,\n",
    "                    groups=self.half_channels, activation=None)\n",
    "            ])\n",
    "\n",
    "        # 信息交换，通道注意力模块\n",
    "        if ca_type == 'se':\n",
    "            self.ca = nn.ModuleList([\n",
    "                SEBlock(channels, internal_neurons=channels // 16),\n",
    "                SEBlock(channels, internal_neurons=channels // 16)])\n",
    "        elif ca_type == 'ca':\n",
    "            self.ca = nn.ModuleList([ChannelAttension(channels),\n",
    "                                     ChannelAttension(channels)])\n",
    "        else:\n",
    "            self.ca = nn.ModuleList([nn.Identity(), nn.Identity()])\n",
    "        \n",
    "        self.nonlinearity = nn.LeakyReLU()\n",
    "        self.conv = RepConv(channels, channels, 1, 1, 0, groups=channels, activation=None)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "        for _b1, _b2, _ca in zip(self.branch1, self.branch2, self.ca):\n",
    "            left, right = torch.chunk(out, 2, dim=1)\n",
    "            left = _b1(left)\n",
    "            right = _b2(right)\n",
    "            out = self.nonlinearity(_ca(torch.cat([left, right], dim=1)))\n",
    "        return self.conv(out + x)\n",
    "\n",
    "\n",
    "class RepBasicUnit(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, ca_type='ca'):\n",
    "        super(RepBasicUnit, self).__init__()\n",
    "        self.left_part = in_channels // 2\n",
    "        self.right_part_in = in_channels - self.left_part\n",
    "        self.right_part_out = out_channels - self.left_part\n",
    "\n",
    "        self.conv1 = nn.Sequential(\n",
    "            RepConv(self.right_part_in, self.right_part_out, kernel=1),\n",
    "            RepConv(self.right_part_out, self.right_part_out, kernel=3,\n",
    "                    padding=1, groups=self.right_part_out, activation=None),\n",
    "        )\n",
    "        self.conv2 = RepConv(out_channels, out_channels, 1, 1, 0, groups=out_channels, activation=nn.LeakyReLU)\n",
    "\n",
    "\n",
    "        if ca_type == 'se':\n",
    "            self.ca = SEBlock(out_channels, internal_neurons=out_channels // 16)\n",
    "        elif ca_type == 'ca':\n",
    "            self.ca = ChannelAttension(out_channels)\n",
    "        elif ca_type == 'none':\n",
    "            self.ca = nn.Identity()\n",
    "        else:\n",
    "            raise ValueError(f'<{ca_type=}> not in se|ca|none')\n",
    "\n",
    "    def forward(self, x):\n",
    "        left = x[:, :self.left_part, :, :]\n",
    "        right = x[:, self.left_part:, :, :]\n",
    "        out = self.conv1(right)\n",
    "        out = self.ca(self.conv2(torch.cat((left, out), 1)))\n",
    "        return out\n",
    "\n",
    "\n",
    "class DWConv_ELAN(nn.Module):\n",
    "    def __init__(self, in_channel, out_channel):\n",
    "        super().__init__()\n",
    "        mid_channel = in_channel // 2\n",
    "        self.conv1 = nn.Sequential(\n",
    "            RepConv(mid_channel, mid_channel, 3, 1, 1, groups=mid_channel, activation=None),\n",
    "            RepConv(mid_channel, mid_channel, 1, 1, 0),\n",
    "            RepConv(mid_channel, mid_channel, 3, 1, 1, groups=mid_channel, activation=None),\n",
    "            RepConv(mid_channel, mid_channel, 1, 1, 0),\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            RepConv(mid_channel, mid_channel, 3, 1, 1, groups=mid_channel, activation=None),\n",
    "            RepConv(mid_channel, mid_channel, 1, 1, 0),\n",
    "            RepConv(mid_channel, mid_channel, 3, 1, 1, groups=mid_channel, activation=None),\n",
    "            RepConv(mid_channel, mid_channel, 1, 1, 0),\n",
    "        )\n",
    "        self.conv3 = nn.Conv2d(4 * mid_channel, out_channel, 1, 1, 0)\n",
    "        self.c = mid_channel\n",
    "\n",
    "    def forward(self, x):\n",
    "        out1 = self.conv1(x[:, :self.c, :, :])\n",
    "        out2 = self.conv2(out1)\n",
    "        out = self.conv3(torch.cat([x, out1, out2], dim=1))\n",
    "        out = channel_shuffle(out, groups=2)\n",
    "        return out\n",
    "\n",
    "\n",
    "class EncoderDecoder(nn.Module):\n",
    "    def __init__(self, num_stage=4, channel=128, ca_type='ca'):\n",
    "        super().__init__()\n",
    "        self.num_stage = num_stage\n",
    "        self.encoder = nn.ModuleList([])\n",
    "        self.decoder = nn.ModuleList([])\n",
    "\n",
    "        self.maxpool = nn.MaxPool2d(2, 2)\n",
    "        for _ in range(num_stage):\n",
    "            self.encoder.append(nn.Sequential(\n",
    "                    RepBasicUnit(channel, channel, ca_type=ca_type),\n",
    "                    RepBasicUnit(channel, channel, ca_type='none'),\n",
    "                    ))\n",
    "            self.decoder.append(nn.Sequential(\n",
    "                    RepBasicUnit(channel, channel, ca_type='none'),\n",
    "                    RepBasicUnit(channel, channel, ca_type=ca_type),\n",
    "                    ))\n",
    "\n",
    "    def forward(self, x):\n",
    "        out_encoder = []   # [128, 64, 32, 16, 8, 4]\n",
    "        out_decoder = []   # [4, 8, 16, 32, 64, 128]\n",
    "\n",
    "        # encoder \n",
    "        for i in range(self.num_stage):\n",
    "            x = self.encoder[i](x)\n",
    "            out_encoder.append(x)\n",
    "            if i != self.num_stage - 1:\n",
    "                x = self.maxpool(x)\n",
    "\n",
    "        # decoder\n",
    "        for i in range(self.num_stage-1, -1, -1):\n",
    "            counterpart = out_encoder[i]\n",
    "            if i == self.num_stage-1:\n",
    "                x = self.decoder[i](counterpart)\n",
    "                h, w = out_encoder[-1].shape[2:]\n",
    "                shortcut = F.adaptive_avg_pool2d(out_encoder[0], (h, w))\n",
    "                x = x + shortcut\n",
    "            else:\n",
    "                x = F.interpolate(x, size=counterpart.shape[2:])\n",
    "                x = x + counterpart\n",
    "                x = self.decoder[i](x)\n",
    "            out_decoder.append(x)\n",
    "        return tuple(out_decoder) \n",
    "\n",
    "\n",
    "class Stem(nn.Module):\n",
    "    def __init__(self, channel, ca_type='ca'):\n",
    "        super().__init__()\n",
    "        mid_channel = max(channel // 4, 32)\n",
    "        self.conv1 = nn.Sequential(\n",
    "            RepConv(3, mid_channel, 3, 2, 1),\n",
    "            RepConv(mid_channel, mid_channel, 7, 1, 3, groups=mid_channel, activation=None)\n",
    "        )\n",
    "        self.branch1 = nn.Sequential(\n",
    "            RepConv(mid_channel, mid_channel, 1, 1, 0),\n",
    "            RepConv(mid_channel, mid_channel, 3, 2, 1,\n",
    "                    groups=mid_channel, activation=None),\n",
    "            RepConv(mid_channel, mid_channel, 1, 1, 0),\n",
    "        )\n",
    "        self.branch2 = nn.MaxPool2d(2, 2, ceil_mode=True)\n",
    "        \n",
    "        self.conv2 = nn.Sequential(\n",
    "            RepConv(2*mid_channel, channel),\n",
    "            MSRB(channel, ca_type=ca_type),\n",
    "            RepConv(channel, channel),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        b1 = self.branch1(out)\n",
    "        b2 = self.branch2(out)\n",
    "        out = self.conv2(torch.cat([b1, b2], dim=1))\n",
    "        return out\n",
    "\n",
    "\n",
    "class LiteHandNet(nn.Module):\n",
    "    def __init__(self, cfg, deploy=False):\n",
    "        super().__init__()\n",
    "        num_stage=cfg.MODEL.get('num_stage', 4)\n",
    "        ca_type=cfg.MODEL.get('ca_type', 'ca')\n",
    "        input_channel=cfg.MODEL.get('input_channel', 256)\n",
    "        output_channel=cfg.MODEL.get('output_channel', cfg.DATASET.num_joints)\n",
    "\n",
    "        self.deploy=deploy\n",
    "        self.stem = Stem(input_channel, ca_type=ca_type)\n",
    "        self.backone = EncoderDecoder(num_stage, input_channel, ca_type=ca_type)\n",
    "\n",
    "        self.neck = nn.Sequential(  \n",
    "                RepBasicUnit(input_channel, input_channel, ca_type=ca_type),\n",
    "                RepConv(input_channel, input_channel, 1, 1, 0),\n",
    "            )\n",
    "        self.head = nn.Conv2d(input_channel, output_channel, 1, 1, 0)\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.stem(x)\n",
    "        out_list = self.backone(out)\n",
    "        out = self.neck(out_list[-1])\n",
    "        out = self.head(out)\n",
    "        return out\n",
    "\n",
    "    def init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                # kaiming_init(m)\n",
    "                normal_init(m)\n",
    "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
    "                constant_init(m, 1)\n",
    "\n",
    "    def deploy_model(self):\n",
    "        for m in self.modules():\n",
    "            if hasattr(m, 'switch_to_deploy'):\n",
    "                m.switch_to_deploy()\n",
    "        self.deploy = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1、LiteHandnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_45670/3619092345.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLiteHandNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maddict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mshow_macs_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdummy_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0minference_speed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdummy_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_45670/1525452496.py\u001b[0m in \u001b[0;36mshow_macs_params\u001b[0;34m(model, img_size, dummy_input)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m      \u001b[0;31m# macs == FLOPS, GFLOPS == 1e12 * FLOPS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mmacs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprofile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdummy_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{model._get_name()}\\t\\t{macs=}\\t{params=}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"FLOPs=\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmacs\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m1e9\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m'{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"G\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/data/miniconda3/envs/TorchCV/lib/python3.8/site-packages/thop/profile.py\u001b[0m in \u001b[0;36mprofile\u001b[0;34m(model, inputs, custom_ops, verbose)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdfs_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\\t\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/data/miniconda3/envs/TorchCV/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_45670/481209571.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m         \u001b[0mout_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mneck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/data/miniconda3/envs/TorchCV/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_45670/481209571.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m         \u001b[0mb1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbranch1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0mb2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbranch2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/data/miniconda3/envs/TorchCV/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/data/miniconda3/envs/TorchCV/lib/python3.8/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/data/miniconda3/envs/TorchCV/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/data/Code/stage-network/models/pose_estimation/liteHandNet/repblocks.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rep_conv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonlinearity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrep_conv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonlinearity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mswitch_to_deploy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/data/miniconda3/envs/TorchCV/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/data/miniconda3/envs/TorchCV/lib/python3.8/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/data/miniconda3/envs/TorchCV/lib/python3.8/site-packages/torchstat/model_hook.py\u001b[0m in \u001b[0;36mwrap_call\u001b[0;34m(module, *input, **kwargs)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0;31m# Itemsize for memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m             \u001b[0mitemsize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitemsize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
     ]
    }
   ],
   "source": [
    "import addict\n",
    "cfg = dict(MODEL=dict(\n",
    "    num_stage=4,\n",
    "    ca_type='ca',  # 'ca' | 'se' | 'none'\n",
    "    input_channel=128,\n",
    "    output_channel=21,\n",
    "))\n",
    "x = torch.rand(1, 3, 256, 256)\n",
    "net = LiteHandNet(addict.Dict(cfg))\n",
    "show_macs_params(net, dummy_input=x)\n",
    "inference_speed(net, dummy_input=x)\n",
    "print()\n",
    "net.deploy_model()\n",
    "show_macs_params(net, dummy_input=x)\n",
    "inference_speed(net, dummy_input=x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2、Hourglass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "\n",
    "Pool = nn.MaxPool2d\n",
    "\n",
    "def batchnorm(x):\n",
    "    return nn.BatchNorm2d(x.size()[1])(x)\n",
    "\n",
    "\n",
    "class Conv(nn.Module):\n",
    "    def __init__(self, inp_dim, out_dim, kernel_size=3, stride = 1, bn = False, relu = True):\n",
    "        super(Conv, self).__init__()\n",
    "        self.inp_dim = inp_dim\n",
    "        self.conv = nn.Conv2d(inp_dim, out_dim, kernel_size, stride, padding=(kernel_size-1)//2, bias=True)\n",
    "        self.relu = None\n",
    "        self.bn = None\n",
    "        if relu:\n",
    "            self.relu = nn.ReLU()\n",
    "        if bn:\n",
    "            self.bn = nn.BatchNorm2d(out_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        assert x.size()[1] == self.inp_dim, \"{} {}\".format(x.size()[1], self.inp_dim)\n",
    "        x = self.conv(x)\n",
    "        if self.bn is not None:\n",
    "            x = self.bn(x)\n",
    "        if self.relu is not None:\n",
    "            x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "class Residual(nn.Module):\n",
    "    def __init__(self, inp_dim, out_dim):\n",
    "        super(Residual, self).__init__()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.bn1 = nn.BatchNorm2d(inp_dim)\n",
    "        self.conv1 = Conv(inp_dim, int(out_dim/2), 1, relu=False)\n",
    "        self.bn2 = nn.BatchNorm2d(int(out_dim/2))\n",
    "        self.conv2 = Conv(int(out_dim/2), int(out_dim/2), 3, relu=False)\n",
    "        self.bn3 = nn.BatchNorm2d(int(out_dim/2))\n",
    "        self.conv3 = Conv(int(out_dim/2), out_dim, 1, relu=False)\n",
    "        if inp_dim == out_dim:\n",
    "            self.skip_layer = nn.Identity()\n",
    "        else:\n",
    "            self.skip_layer = Conv(inp_dim, out_dim, 1, relu=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        residual = self.skip_layer(x)\n",
    "        out = self.bn1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv1(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn3(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv3(out)\n",
    "        out += residual\n",
    "        return out \n",
    "\n",
    "class HourglassModule(nn.Module):\n",
    "    def __init__(self, n, f, bn=None, increase=0):\n",
    "        super(HourglassModule, self).__init__()\n",
    "        nf = f + increase\n",
    "        self.up1 = Residual(f, f)\n",
    "        # Lower branch\n",
    "        self.pool1 = Pool(2, 2)\n",
    "        self.low1 = Residual(f, nf)\n",
    "        self.n = n\n",
    "        # Recursive hourglass\n",
    "        if self.n > 1:\n",
    "            self.low2 = HourglassModule(n-1, nf, bn=bn)\n",
    "        else:\n",
    "            self.low2 = Residual(nf, nf)\n",
    "        self.low3 = Residual(nf, f)\n",
    "        self.up2 = nn.Upsample(scale_factor=2, mode='nearest')\n",
    "\n",
    "    def forward(self, x):\n",
    "        up1  = self.up1(x)\n",
    "        pool1 = self.pool1(x)\n",
    "        low1 = self.low1(pool1)\n",
    "        low2 = self.low2(low1)\n",
    "        low3 = self.low3(low2)\n",
    "        up2  = self.up2(low3)\n",
    "        return up1 + up2\n",
    "\n",
    "class Merge(nn.Module):\n",
    "    def __init__(self, x_dim, y_dim):\n",
    "        super(Merge, self).__init__()\n",
    "        self.conv = Conv(x_dim, y_dim, 1, relu=False, bn=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class HourglassNet(nn.Module):\n",
    "    \"\"\"https://github.com/princeton-vl/pytorch_stacked_hourglass\"\"\"\n",
    "    def __init__(self, cfg):\n",
    "        super(HourglassNet, self).__init__()\n",
    "        num_stack = cfg.MODEL.get('num_stack', 8)\n",
    "        num_level = cfg.MODEL.get('num_level', 4)\n",
    "        inp_dim = cfg.MODEL.get('input_channel', 256)\n",
    "        oup_dim = cfg.MODEL.get('output_channel', 21)\n",
    "\n",
    "        self.num_stack = num_stack\n",
    "        self.pre = nn.Sequential(\n",
    "            Conv(3, 64, 7, 2, bn=True, relu=True),\n",
    "            Residual(64, 128),\n",
    "            Pool(2, 2),\n",
    "            Residual(128, 128),\n",
    "            Residual(128, inp_dim)\n",
    "        )\n",
    "\n",
    "        self.hgs = nn.ModuleList( [\n",
    "        nn.Sequential(\n",
    "            HourglassModule(num_level, inp_dim, bn=False, increase=0),\n",
    "        ) for _ in range(num_stack)] )\n",
    "\n",
    "        self.features = nn.ModuleList([\n",
    "        nn.Sequential(\n",
    "            Residual(inp_dim, inp_dim),\n",
    "            Conv(inp_dim, inp_dim, 1, bn=True, relu=True)\n",
    "        ) for _ in range(num_stack)] )\n",
    "\n",
    "        self.outs = nn.ModuleList([Conv(inp_dim, oup_dim, 1, relu=False, bn=False) for i in range(num_stack)] )\n",
    "        self.merge_features = nn.ModuleList([Merge(inp_dim, inp_dim) for _ in range(num_stack-1)] )\n",
    "        self.merge_preds = nn.ModuleList( [Merge(oup_dim, inp_dim) for _ in range(num_stack-1)] )\n",
    "        self.num_stack = num_stack\n",
    "\n",
    "    def forward(self, imgs):\n",
    "        ## our posenet\n",
    "        # x = imgs.permute(0, 3, 1, 2) #x of size 1,3,inpdim,inpdim\n",
    "        x = self.pre(imgs)\n",
    "        outs = []\n",
    "        for i in range(self.num_stack):\n",
    "            hg = self.hgs[i](x)\n",
    "            feature = self.features[i](hg)\n",
    "            preds = self.outs[i](feature)\n",
    "            outs.append(preds)\n",
    "            if i < self.num_stack - 1:\n",
    "                x = x + self.merge_preds[i](preds) + self.merge_features[i](feature)\n",
    "        return torch.stack(outs, dim=1)  # [N, num_stack, K, H, W]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 hourglass test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HourglassNet\t\tmacs=5202833408.0\tparams=3427733.0\n",
      "FLOPs= 5.202833408G\tparams= 3.427733M\n",
      "macs='5.203G'\tparams='3.428M'\n",
      "Mean@ 9.872ms Std@ 0.411ms FPS@ 101.30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import addict\n",
    "cfg = dict(MODEL=dict(\n",
    "    num_stack=1,\n",
    "    num_level=4,\n",
    "    input_channel=256,\n",
    "    output_channel=21,\n",
    "))\n",
    "x = torch.rand(1, 3, 256, 256)\n",
    "net = HourglassNet(addict.Dict(cfg))\n",
    "show_macs_params(net, dummy_input=x)\n",
    "inference_speed(net, dummy_input=x)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from models import kaiming_init, constant_init, normal_init\n",
    "from models.pose_estimation.liteHandNet.common import SEBlock, channel_shuffle, ChannelAttension\n",
    "from models.pose_estimation.liteHandNet.repblocks import RepConv, RepBlock\n",
    "\n",
    "class DWConv(nn.Module):\n",
    "    \"\"\"DepthwiseSeparableConvModul 深度可分离卷积\"\"\"\n",
    "    def __init__(self, in_channel, out_channel, stride=1, padding=1, dilation=1,\n",
    "                 activation=nn.LeakyReLU):\n",
    "        super().__init__()\n",
    "        self.depthwise_conv = RepConv(in_channel, in_channel, 3, stride, padding,\n",
    "                                       groups=in_channel, dilation=dilation,\n",
    "                                       activation=activation, inplace=False)\n",
    "        self.pointwise_conv = RepConv(in_channel, out_channel, 1, 1, 0,\n",
    "                                      activation=activation, inplace=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.depthwise_conv(x)\n",
    "        out = self.pointwise_conv(out)\n",
    "        return out\n",
    " \n",
    "class BottleNeck(nn.Module):\n",
    "    \"\"\"用于提高深度,但尽可能少地增加运算量, 不改变通道数\"\"\"\n",
    "    def __init__(self, channel, reduction=4, activation=nn.LeakyReLU):\n",
    "        super(BottleNeck, self).__init__()\n",
    "        mid_channel = channel // reduction\n",
    "        self.conv = nn.Sequential(\n",
    "            RepConv(channel, mid_channel, 1, 1, 0,\n",
    "                    activation=activation, inplace=True),\n",
    "            RepBlock(mid_channel, mid_channel, 3, 1, 1,\n",
    "                     activation=activation, inplace=True, identity=False),\n",
    "            RepConv(mid_channel, channel, 1, 1, 0,\n",
    "                    activation=None),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return F.relu(x + self.conv(x))\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, inp_dim, out_dim, stride=1, activation=nn.LeakyReLU):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            RepConv(inp_dim, out_dim, 3, stride, 1,\n",
    "                     activation=activation, inplace=True),\n",
    "            RepBlock(inp_dim, out_dim, 3, 1, 1,\n",
    "                     activation=None, identity=False)\n",
    "        )\n",
    "        if stride == 2 or inp_dim != out_dim:\n",
    "            self.skip_layer = RepConv(inp_dim, out_dim, 1, stride, 0, activation=None)\n",
    "        else:\n",
    "            self.skip_layer = nn.Identity()\n",
    "    def forward(self, x):\n",
    "        return F.relu(self.skip_layer(x) + self.conv(x))\n",
    "\n",
    "\n",
    "class Residual(nn.Module):\n",
    "    def __init__(self, inp_dim, out_dim, stride=2, num_block=2,\n",
    "                 reduction=2, activation=nn.LeakyReLU):\n",
    "        super().__init__()\n",
    "        self.conv1 = BasicBlock(inp_dim, out_dim, stride, activation)\n",
    "        self.blocks = nn.Sequential(\n",
    "            *[BottleNeck(out_dim, reduction, activation) for _ in range(num_block)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.blocks(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class EncoderDecoder(nn.Module):\n",
    "    def __init__(self, num_levels=5, inp_dim=128, num_blocks=[],\n",
    "                 ca_type='ca', reduction=2, activation=nn.LeakyReLU):\n",
    "        super().__init__()\n",
    "        self.num_levels = num_levels\n",
    "        self.encoder = nn.ModuleList([])\n",
    "        self.decoder = nn.ModuleList([])\n",
    "        assert len(num_blocks) == num_levels - 1\n",
    "\n",
    "        self.encoder.append(MSAB(inp_dim, inp_dim, ca_type=ca_type))\n",
    "        for i in range(num_levels-1):\n",
    "            self.encoder.append(\n",
    "                Residual(inp_dim, inp_dim, 2, num_blocks[i], reduction, activation))\n",
    "            self.decoder.append(\n",
    "                Residual(inp_dim, inp_dim, 1, num_blocks[i], reduction, activation))\n",
    "        self.decoder.append(MSAB(inp_dim, inp_dim, ca_type=ca_type))\n",
    "\n",
    "    def forward(self, x):\n",
    "        out_encoder = []   # [128, 64, 32, 16, 8, 4]\n",
    "        out_decoder = []   # [4, 8, 16, 32, 64, 128]\n",
    "\n",
    "        # encoder \n",
    "        for encoder_layer in self.encoder:\n",
    "            x = encoder_layer(x)\n",
    "            out_encoder.append(x)\n",
    "\n",
    "        # ! 我觉得只添加一次简单的shortcut就够了\n",
    "        h, w = out_encoder[-1].shape[2:]\n",
    "        shortcut = F.adaptive_avg_pool2d(out_encoder[0], (h, w))\n",
    "\n",
    "        # decoder  \n",
    "        for i, decoder_layer in enumerate(self.decoder):\n",
    "            counterpart = out_encoder[self.num_levels-1-i]\n",
    "            if i == 0:\n",
    "                x = decoder_layer(counterpart)\n",
    "                x = x + shortcut\n",
    "            else:\n",
    "                h, w = counterpart.shape[2:]\n",
    "                x = decoder_layer(x)\n",
    "                x = F.interpolate(x, size=(h, w))\n",
    "                x = x + counterpart\n",
    "            out_decoder.append(x)\n",
    "        return tuple(out_decoder) \n",
    "\n",
    "\n",
    "class MSAB(nn.Module):\n",
    "    \"\"\"\n",
    "    https://blog.csdn.net/KevinZ5111/article/details/104730835?utm_medium=distribute.pc_aggpage_search_result.none-task-blog-2~aggregatepage~first_rank_ecpm_v1~rank_v31_ecpm-4-104730835.pc_agg_new_rank&utm_term=block%E6%94%B9%E8%BF%9B+residual&spm=1000.2123.3001.4430\n",
    "    \"\"\"\n",
    "    def __init__(self, in_c, out_c, ca_type='ca', activation=nn.LeakyReLU):\n",
    "        super().__init__()\n",
    "\n",
    "        mid_c = in_c // 2\n",
    "        self.conv1 = RepConv(in_c, mid_c, 1, 1, 0, activation=activation, inplace=True)\n",
    "\n",
    "        self.mid1_conv = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                DWConv(mid_c, mid_c // 2, activation=activation),\n",
    "                DWConv(mid_c // 2, mid_c // 2, activation=activation)\n",
    "            ), \n",
    "            nn.Sequential(\n",
    "                DWConv(mid_c, mid_c, activation=activation),\n",
    "                DWConv(mid_c, mid_c, activation=activation),        \n",
    "            )])\n",
    "\n",
    "        self.mid2_conv = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                DWConv(mid_c, mid_c // 2, dilation=2, padding=2, activation=activation),\n",
    "                DWConv(mid_c // 2, mid_c // 2, activation=activation)),\n",
    "            nn.Sequential(\n",
    "                DWConv(mid_c, mid_c, dilation=2, padding=2, activation=activation),\n",
    "                DWConv(mid_c, mid_c, activation=activation))\n",
    "            ])\n",
    "\n",
    "        self.conv2 = RepConv(in_c, out_c, 1, 1, 0, activation=activation, inplace=True)\n",
    "        \n",
    "        if ca_type == 'se':\n",
    "            self.ca = SEBlock(out_c, internal_neurons=out_c // 16)\n",
    "        elif ca_type == 'ca':\n",
    "            self.ca = ChannelAttension(out_c)\n",
    "        elif ca_type == 'none':\n",
    "            self.ca = nn.Identity()\n",
    "        else:\n",
    "            raise ValueError(f'<{ca_type=}> not in se|ca|none')\n",
    "\n",
    "    def forward(self, x):\n",
    "        m = self.conv1(x)\n",
    "        for i in range(2):\n",
    "            m1 = self.mid1_conv[i](m)\n",
    "            m2 = self.mid2_conv[i](m)\n",
    "            m = torch.cat([m1, m2], dim=1)\n",
    "\n",
    "        features = m + x\n",
    "        out = self.conv2(features)\n",
    "        out = self.ca(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Stem(nn.Module):\n",
    "    \"\"\" 我在Conv1中再加了一个3x3卷积, 来提高stem的初始感受野\"\"\"\n",
    "    def __init__(self, out_channel=256, min_mid_c=32, activation=nn.LeakyReLU):\n",
    "        super().__init__()\n",
    "        mid_channel = out_channel // 4 if out_channel // 4 >= min_mid_c else min_mid_c\n",
    "\n",
    "        self.conv1 = nn.Sequential(\n",
    "            RepBlock(3, mid_channel, 3, 2, 1, activation=activation, inplace=True),\n",
    "            RepBlock(mid_channel, mid_channel, 7, 1, 3, groups=mid_channel,\n",
    "                     activation=activation, inplace=True)\n",
    "        )\n",
    "        self.branch1 = nn.Sequential(\n",
    "            RepConv(mid_channel, mid_channel, 1, 1, 0, activation=activation, inplace=True),\n",
    "            RepConv(mid_channel, mid_channel, 3, 2, 1, activation=activation, inplace=True),\n",
    "        )\n",
    "        self.branch2 = nn.MaxPool2d(2, 2, ceil_mode=True)\n",
    "        self.conv1x1 = nn.Conv2d(mid_channel * 2, out_channel, 1, 1, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        b1 = self.branch1(out)\n",
    "        b2 = self.branch2(out)\n",
    "        out = torch.cat([b1, b2], dim=1)\n",
    "        out = self.conv1x1(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class LiteHandNet(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        num_stage=cfg.MODEL.get('num_stage', 4)\n",
    "        inp_dim=cfg.MODEL.get('input_channel', 128)\n",
    "        oup_dim=cfg.MODEL.get('output_channel', cfg.DATASET.num_joints)\n",
    "        num_block=cfg.MODEL.get('num_block', [2, 2, 2])\n",
    "        ca_type = cfg.MODEL.get('ca_type', 'ca')  # 'ca' | 'se' | 'none'\n",
    "        reduction = cfg.MODEL.get('reduction', 2)\n",
    "        activation = cfg.MODEL.get('activation', nn.LeakyReLU)\n",
    "        assert reduction in [2, 4]\n",
    "        assert ca_type in ['ca', 'se', 'none']\n",
    "\n",
    "        self.pre = Stem(inp_dim, activation=activation)\n",
    "        self.hgs = EncoderDecoder(num_stage, inp_dim, num_block,\n",
    "                                  ca_type, reduction, activation) \n",
    "\n",
    "        self.features = nn.Sequential(\n",
    "                BottleNeck(inp_dim, 2, activation),\n",
    "                RepConv(inp_dim, inp_dim, 1, 1, 0, activation=activation, inplace=True),\n",
    "            )\n",
    "\n",
    "        self.out_layer = nn.Conv2d(inp_dim, oup_dim, 1, 1, 0)\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, imgs):\n",
    "        # our posenet\n",
    "        x = self.pre(imgs)\n",
    "        hg = self.hgs(x)\n",
    "        feature = self.features(hg[-1])\n",
    "        preds = self.out_layer(feature)\n",
    "        return preds\n",
    "\n",
    "    def init_weights(self):\n",
    "        for m in self.modules():\n",
    "            normal_init(m)\n",
    "\n",
    "    def deploy_model(self):\n",
    "        for m in self.modules():\n",
    "            if hasattr(m, 'switch_to_deploy'):\n",
    "                m.switch_to_deploy()\n",
    "        self.deploy = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3、MyNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiScaleAttentionHourglass\t\tmacs=1142708096.0\tparams=2294357.0\n",
      "FLOPs= 1.142708096G\tparams= 2.294357M\n",
      "macs='1.143G'\tparams='2.294M'\n",
      "Mean@ 20.757ms Std@ 2.145ms FPS@ 48.18\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import addict\n",
    "from models import mynet\n",
    "stage = 4\n",
    "cfg = dict(MODEL=dict(\n",
    "    num_stage=stage,\n",
    "    num_block=[2, 3, 4],\n",
    "    input_channel=128,\n",
    "    ca_type='ca',\n",
    "    reduction=4,\n",
    "    activation=nn.LeakyReLU,\n",
    "    output_channel=21,\n",
    "))\n",
    "x = torch.rand(1, 3, 256, 256)\n",
    "net = mynet(addict.Dict(cfg))\n",
    "show_macs_params(net, dummy_input=x)\n",
    "inference_speed(net, dummy_input=x)\n",
    "print()\n",
    "\n",
    "# net = LiteHandNet(addict.Dict(cfg))\n",
    "# net.eval()\n",
    "# y = net(x)\n",
    "# show_macs_params(net, dummy_input=x)\n",
    "# inference_speed(net, dummy_input=x)\n",
    "# print()\n",
    "\n",
    "# net.deploy_model()\n",
    "# show_macs_params(net, dummy_input=x)\n",
    "# inference_speed(net, dummy_input=x)\n",
    "# print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('TorchCV')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "435570d4219e70938454f0c8f629267d4bfa46e86b2ba3c4b1d73b5202317604"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
