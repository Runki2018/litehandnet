{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from config import dataset_dict\n",
    "from shutil import copy\n",
    "import json\n",
    "import cv2\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from utils.data_augmentation import adjust_gamma, homography, horizontal_flip\n",
    "from data.handset.dataset_function import validate_keypoints, validate_label, get_bbox\n",
    "from tqdm import tqdm\n",
    "# from utils.visualization_tools import draw_bbox, draw_point\n",
    "\n",
    "\n",
    "def data_augmentation(image, keypoints, bbox, prob=0.5):\n",
    "    \"\"\" data augmentation for specified image\n",
    "\n",
    "    Args:\n",
    "        image (np.ndarray): image read by cv2 \n",
    "        keypoints (list): [x1, y1 ,v1, ... ,x21, y21, v21]\n",
    "        bbox (list): [lx, ly, w, h]\n",
    "        prob (float): the probability of data augmentation\n",
    "    \"\"\"\n",
    "    H, W, C = image.shape\n",
    "    if image is None:\n",
    "            raise ValueError(' Fail to read %s' % image)\n",
    "    keypoints = np.array(keypoints, dtype=np.float32).reshape((21, 3))\n",
    "    bbox = np.array(bbox, dtype=np.float32)\n",
    "\n",
    "    # get crop region\n",
    "    x, y, w, h = bbox  # topleft_x, topleft_y, width, height\n",
    "    scale_factor = np.random.random()\n",
    "    dx = (scale_factor * w) // 2\n",
    "    dy = (scale_factor * h) // 2\n",
    "    \n",
    "    x1 = max(0, int(x - dx))\n",
    "    y1 = max(0, int(y - dy))\n",
    "    x2 = min(W, int(x + w + dx)) \n",
    "    y2 = min(H, int(y + h + dy)) \n",
    "    \n",
    "    h_crop, w_crop = y2 - y1, x2 - x1\n",
    "\n",
    "    img_crop = image[y1:y2, x1:x2]\n",
    "    image = cv2.resize(img_crop, (W, H), interpolation=cv2.INTER_NEAREST)\n",
    "    \n",
    "    scale_factor = W / w_crop, H / h_crop\n",
    "    keypoints[:, :2] -= [x1, y1]\n",
    "    keypoints[:, :2] *= scale_factor\n",
    "    bbox[:2] -= [x1, y1]\n",
    "    bbox[:2] *= scale_factor  \n",
    "    bbox[2:] *= scale_factor  \n",
    "    \n",
    "    image = adjust_gamma(image, prob=0.5)\n",
    "    image, keypoints, bbox = homography(image, keypoints[None],\n",
    "                                        prob=prob, bbox=bbox[None],\n",
    "                                        topleft=True)\n",
    "    image, keypoints, bbox = horizontal_flip(image, keypoints,\n",
    "                                                prob=prob, bbox=bbox)\n",
    "    # 验证关键点是否可见，不可见设置为-1     \n",
    "    keypoints = validate_keypoints(keypoints, img_size=[H, W])  \n",
    "    keypoints = keypoints[0]  # (1, 21, 3) -> (21, 3) \n",
    "    bbox = bbox[0].tolist()\n",
    "    \n",
    "    # image = draw_point(image, keypoints)\n",
    "    keypoints = keypoints.reshape((keypoints.size)).tolist()\n",
    "   \n",
    "    # bbox = bbox.tolist()\n",
    "    # image = draw_bbox(image, bbox[0], bbox[1], bbox[0]+bbox[2], bbox[1]+bbox[3])\n",
    "    return image, keypoints, bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_freiahand_multiscale_dataset(save_root='./'):\n",
    "    \n",
    "    freihand = dataset_dict['freihand']\n",
    "    freihand_root = freihand['root']\n",
    "    test_file = freihand['test_file']\n",
    "    train_file = freihand['train_file']\n",
    "    val_file = freihand['val_file']\n",
    "    \n",
    "    new_ann_dir = os.path.join(save_root, \"annotations\")\n",
    "    new_images_dir = os.path.join(save_root, \"images\")\n",
    "    new_test_file = os.path.join(new_ann_dir, \"freihand_test.json\") \n",
    "    new_train_val_file = os.path.join(new_ann_dir, \"freihand_train_val.json\")\n",
    "\n",
    "    config_info = dict(\n",
    "        name=\"freihand_plus\",\n",
    "        root=save_root,\n",
    "        test_file=new_test_file,      \n",
    "        train_file=new_train_val_file,\n",
    "    )\n",
    "\n",
    "    if not os.path.exists(save_root):\n",
    "        os.makedirs(save_root)\n",
    "        print(\"mkdir save_root : {}\".format(save_root))\n",
    "    \n",
    "    if not os.path.exists(new_ann_dir):\n",
    "        os.mkdir(new_ann_dir)\n",
    "        print(\"mkdir new_ann_path : {}\".format(save_root))\n",
    "     \n",
    "    if not os.path.exists(new_images_dir):\n",
    "        os.mkdir(new_images_dir)\n",
    "        print(\"mkdir new_images_path : {}\".format(save_root))\n",
    "    \n",
    "    config_save_path = os.path.join(save_root, \"config_info.json\")\n",
    "    json.dump(config_info, open(config_save_path, 'w'), indent=4)\n",
    "    \n",
    "\n",
    "    test_dict = json.load(open(test_file, 'r'))\n",
    "    # move test set \n",
    "    g_imgs = []\n",
    "    for img_info in tqdm(test_dict['images'], desc='move test set '):\n",
    "        # copy image files to new directory\n",
    "        file_name = img_info['file_name']\n",
    "        base_file_name = os.path.basename(file_name)\n",
    "        \n",
    "        image_path = os.path.join(freihand_root, img_info['file_name'])\n",
    "        copy(\n",
    "                image_path,\n",
    "                os.path.join(new_images_dir, base_file_name) \n",
    "            )\n",
    "        \n",
    "        img_info['file_name'] = \"images/\" + base_file_name\n",
    "        g_imgs.append(img_info)\n",
    "\n",
    "    test_dict['images'] = g_imgs\n",
    "    json.dump(test_dict, open(new_test_file, 'w'))\n",
    "\n",
    "\n",
    "    # merge train_set and val_set\n",
    "    g_imgs = []\n",
    "    g_anns = []\n",
    "\n",
    "    train_dict = json.load(open(train_file, 'r'))\n",
    "    val_dict = json.load(open(val_file, 'r'))\n",
    "    \n",
    "    images_list = train_dict['images'] + val_dict['images']\n",
    "    anns_list = train_dict['annotations'] + val_dict['annotations']\n",
    "    \n",
    "    assert len(images_list) == len(anns_list), \\\n",
    "        \"{} <> {}\".format(len(images_list), len(anns_list))\n",
    "        \n",
    "    max_id = max([img['id'] for img in images_list])\n",
    "    total_new = 0\n",
    "    for i  in tqdm(range(len(images_list)),\n",
    "                   desc='merge train_set and val_set'):\n",
    "        img_info = images_list[i]\n",
    "        ann_info = anns_list[i]\n",
    "        \n",
    "        img_copy = deepcopy(img_info)\n",
    "        ann_copy = deepcopy(ann_info)\n",
    "        # copy image files to new directory\n",
    "        file_name = img_copy['file_name']\n",
    "        base_file_name = os.path.basename(file_name)\n",
    "        \n",
    "        image_path = os.path.join(freihand_root, img_copy['file_name'])\n",
    "        copy(\n",
    "                image_path,\n",
    "                os.path.join(new_images_dir, base_file_name) \n",
    "            )\n",
    "           \n",
    "        img_copy['file_name'] = \"images/\" + base_file_name\n",
    "        g_imgs.append(img_copy)\n",
    "        g_anns.append(ann_copy)\n",
    "        \n",
    "        # generate mutiscale samples\n",
    "        if np.random.random() < 0.5:\n",
    "            total_new += 1\n",
    "            \n",
    "            img_copy = deepcopy(img_info)\n",
    "            ann_copy = deepcopy(ann_info)\n",
    "            \n",
    "            image = cv2.imread(image_path)\n",
    "            keypoints = ann_copy['keypoints']\n",
    "            bbox = ann_copy['bbox']\n",
    "            \n",
    "            image, keypoints, bbox = \\\n",
    "                data_augmentation(image, keypoints, bbox, prob=0)\n",
    "            \n",
    "            new_file_name = \"ms_\" + base_file_name\n",
    "            # record\n",
    "            new_id = max_id + total_new\n",
    "            img_copy['file_name'] = \"images/\" +  new_file_name\n",
    "            img_copy['id'] = new_id\n",
    "            \n",
    "            ann_copy['keypoints'] = keypoints\n",
    "            ann_copy['bbox'] = bbox\n",
    "            ann_copy['id'] = new_id\n",
    "            ann_copy['image_id'] = new_id\n",
    "            \n",
    "            save_img_path = os.path.join(new_images_dir, new_file_name)\n",
    "            flag = cv2.imwrite(save_img_path, image)\n",
    "            if not flag:\n",
    "                raise ValueError(\"image save error!!!\")\n",
    "\n",
    "            g_imgs.append(img_copy)\n",
    "            g_anns.append(ann_copy)\n",
    "    \n",
    "    train_dict['images'] = g_imgs \n",
    "    train_dict['annotations'] = g_anns\n",
    "    json.dump(train_dict, open(new_train_val_file, 'w'))\n",
    "    \n",
    "    print(\"work done successfully\")\n",
    "    print(\"the number of test_set: \\t\\t{}\".format(len(test_dict['images'])))\n",
    "    print(\"the number of new samples: \\t\\t {}\".format(total_new))\n",
    "    print(\"the number of train_val_set: \\t\\t{}\".format(len(g_anns)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir save_root : /root/data/Dataset/freihand_plus\n",
      "mkdir new_ann_path : /root/data/Dataset/freihand_plus\n",
      "mkdir new_images_path : /root/data/Dataset/freihand_plus\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "move test set : 100%|██████████| 13024/13024 [00:32<00:00, 395.25it/s]\n",
      "merge train_set and val_set: 100%|██████████| 117216/117216 [18:10<00:00, 107.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "work done successfully\n",
      "the number of test_set: \t\t13024\n",
      "the number of new samples: \t\t 58507\n",
      "the number of train_val_set: \t\t175723\n"
     ]
    }
   ],
   "source": [
    "generate_freiahand_multiscale_dataset(\n",
    "    save_root='/root/data/Dataset/freihand_plus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "435570d4219e70938454f0c8f629267d4bfa46e86b2ba3c4b1d73b5202317604"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('TorchCV')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
