{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "初始化测试函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np \n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import addict\n",
    "%matplotlib inline\n",
    "from thop import profile\n",
    "from thop import clever_format\n",
    "from torchstat import stat\n",
    "\n",
    "def show_model_stat(model, img_size=(3, 256, 256)):\n",
    "    stat(model, img_size)\n",
    "\n",
    "\n",
    "def show_macs_params(model, img_size=(256, 256), dummy_input=None):\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    if dummy_input is None:\n",
    "        dummy_input = torch.randn(1, 3, img_size[0], img_size[1], dtype=torch.float)\n",
    "    dummy_input = dummy_input.to(device)\n",
    "    \n",
    "     # macs == FLOPS, GFLOPS == 1e12 * FLOPS\n",
    "    macs, params = profile(model, inputs=(dummy_input,), verbose=False) \n",
    "    print(f\"{model._get_name()}\\t\\t{macs=}\\t{params=}\")\n",
    "    print(\"FLOPs=\", str(macs/1e9) +'{}'.format(\"G\"), end='\\t')\n",
    "    print(\"params=\", str(params/1e6)+'{}'.format(\"M\"))\n",
    "    macs, params = clever_format([macs, params], \"%.3f\")\n",
    "    print(f\"{macs=}\\t{params=}\")\n",
    "\n",
    "\n",
    "def inference_speed(model, img_size=(256, 256), dummy_input=None, repetitions=10000):\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    if dummy_input is None:\n",
    "        dummy_input = torch.randn(1, 3, img_size[0], img_size[1], dtype=torch.float)\n",
    "    dummy_input = dummy_input.to(device)\n",
    "\n",
    "    starter = torch.cuda.Event(enable_timing=True)\n",
    "    ender = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "    \n",
    "    timings=np.zeros((repetitions,1))\n",
    "    #GPU-WARM-UP\n",
    "    for _ in range(10):\n",
    "        _ = model(dummy_input)\n",
    "    # MEASURE PERFORMANCE\n",
    "    with torch.no_grad():\n",
    "        for rep in range(repetitions):\n",
    "            starter.record()\n",
    "            _ = model(dummy_input)\n",
    "            ender.record()\n",
    "            # WAIT FOR GPU SYNC\n",
    "            torch.cuda.synchronize()\n",
    "            curr_time = starter.elapsed_time(ender)\n",
    "            timings[rep] = curr_time\n",
    "    mean_syn = np.sum(timings) / repetitions\n",
    "    std_syn = np.std(timings)\n",
    "    mean_fps = 1000. / mean_syn\n",
    "    print('Mean@ {mean_syn:.3f}ms Std@ {std_syn:.3f}ms FPS@ {mean_fps:.2f}'\\\n",
    "        .format(mean_syn=mean_syn, std_syn=std_syn, mean_fps=mean_fps))\n",
    "    # ! @n 中的n是什么意思？\n",
    "    # print(' * Mean@1 {mean_syn:.3f}ms Std@5 {std_syn:.3f}ms FPS@1 {mean_fps:.2f}'\\\n",
    "    #     .format(mean_syn=mean_syn, std_syn=std_syn, mean_fps=mean_fps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_dict = dict(\n",
    "    mynet=dict(MODEL=dict(name='mynet',\n",
    "                          input_channel=128,\n",
    "                          output_channel=21,       # num_joints + 3 region map\n",
    "                          num_stage=4,\n",
    "                          num_block=[2, 2, 2],\n",
    "                          )),\n",
    "    litehandnet=dict(MODEL=dict(\n",
    "            name='litehandnet',\n",
    "            num_stage=4,\n",
    "            num_block=[2, 2, 2],\n",
    "            input_channel=128,\n",
    "            ca_type='ca',\n",
    "            reduction=4,\n",
    "            activation=\"leakyrelu\", # 'leakyrelu', 'relu', 'silu'\n",
    "            output_channel=21,\n",
    "    )),\n",
    "    resnet18=dict(MODEL=dict(\n",
    "                        name='resnet',\n",
    "                        depth=18,\n",
    "                        output_channel=21,  # num_joints + 3 region map\n",
    "                        stem_channels=64,\n",
    "                        base_channels=64,\n",
    "                        strides=(1, 2, 2, 2),\n",
    "                        deep_stem=False,         # stem是否用三个3x3卷积代替7x7卷积\n",
    "                        num_stages=4,\n",
    "                        out_indices=(3,),         # (0, 1, 2, 3)\n",
    "                        )),\n",
    "    resnet50=dict(MODEL=dict(\n",
    "                        name='resnet',\n",
    "                        depth=50,\n",
    "                        output_channel=21,  # num_joints + 3 region map\n",
    "                        stem_channels=64,\n",
    "                        base_channels=64,\n",
    "                        strides=(1, 2, 2, 2),\n",
    "                        deep_stem=False,         # stem是否用三个3x3卷积代替7x7卷积\n",
    "                        num_stages=4,\n",
    "                        out_indices=(3,),         # (0, 1, 2, 3)\n",
    "                        )),\n",
    "    mobilenetv2=dict(MODEL=dict(\n",
    "                        name='mobilenetv2',\n",
    "                        widen_factor=1,\n",
    "                        out_indices=(7,),\n",
    "                        output_channel=21,\n",
    "                        )),\n",
    "    litehrnet18=dict(MODEL=dict(\n",
    "                        name='litehrnet',\n",
    "                        depth=18,\n",
    "                        output_channel=21,  # num_joints + 3 region map\n",
    "                        )),\n",
    "    litehrnet30=dict(MODEL=dict(\n",
    "                        name='litehrnet',\n",
    "                        depth=30,\n",
    "                        output_channel=21,  # num_joints + 3 region map\n",
    "                        )),\n",
    "    hourglass_2=dict(MODEL=dict(\n",
    "                        name='hourglass',\n",
    "                        input_channel=256,\n",
    "                        output_channel=21,       # num_joints + 3 region map\n",
    "                        num_stack=2,            # 沙漏模块的个数\n",
    "                        num_level=4,            # 每个沙漏模块不同尺度特征层个数\n",
    "                        )),\n",
    "    hourglass_1=dict(MODEL=dict(\n",
    "                        name='hourglass',\n",
    "                        input_channel=256,\n",
    "                        output_channel=21,       # num_joints + 3 region map\n",
    "                        num_stack=1,            # 沙漏模块的个数\n",
    "                        num_level=4,            # 每个沙漏模块不同尺度特征层个数\n",
    "                        )),\n",
    "    srhandnet=dict(MODEL=dict(\n",
    "                        name='srhandnet',\n",
    "                        output_channel=21,  # num_joints + 3 region map\n",
    "                        )),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1、测试所有模型的参数和推理速度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import srhandnet, litehrnet, mynet, resnet, mobilenetv2, hourglass, litehandnet\n",
    "from copy import deepcopy\n",
    "\n",
    "# for img_size in [(224, 224), (256, 256)]:\n",
    "#     for model_name, cfg in cfg_dict.items():\n",
    "#         print(f\"{model_name}\\t{img_size}\".center(80, '-'))\n",
    "#         if \"hourglass\" in model_name and img_size == (224, 224):\n",
    "#             cfg = deepcopy(cfg)\n",
    "#             cfg['MODEL']['num_level'] = 3\n",
    "#         cfg = addict.Dict(cfg)\n",
    "#         model = eval(cfg.MODEL.name)(cfg)\n",
    "#         show_macs_params(model, img_size=img_size)\n",
    "#         inference_speed(model, img_size=img_size)\n",
    "#         print()\n",
    "#         if model_name == 'litehandnet':\n",
    "#             for m in model.modules():\n",
    "#                 if hasattr(m, 'switch_to_deploy'):\n",
    "#                     m.switch_to_deploy()\n",
    "#             show_macs_params(model, img_size=img_size)\n",
    "#             inference_speed(model, img_size=img_size)\n",
    "#             print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2、测试所有模型在同一输入下的参数和推理速度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------mynet--------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/data/miniconda3/envs/TorchCV/lib/python3.8/site-packages/thop/vision/basic_hooks.py:92: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  kernel = torch.DoubleTensor([*(x[0].shape[2:])]) // torch.DoubleTensor(list((m.output_size,))).squeeze()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiScaleAttentionHourglass\t\tmacs=1135802240.0\tparams=2240405.0\n",
      "FLOPs= 1.13580224G\tparams= 2.240405M\n",
      "macs='1.136G'\tparams='2.240M'\n",
      "Mean@ 71.053ms Std@ 20.139ms FPS@ 14.07\n",
      "\n",
      "----------------------------------litehandnet-----------------------------------\n",
      "LiteHandNet\t\tmacs=1312865536.0\tparams=2272981.0\n",
      "FLOPs= 1.312865536G\tparams= 2.272981M\n",
      "macs='1.313G'\tparams='2.273M'\n",
      "Mean@ 76.100ms Std@ 19.763ms FPS@ 13.14\n",
      "\n",
      "LiteHandNet\t\tmacs=1297824768.0\tparams=2265621.0\n",
      "FLOPs= 1.297824768G\tparams= 2.265621M\n",
      "macs='1.298G'\tparams='2.266M'\n",
      "Mean@ 63.758ms Std@ 21.498ms FPS@ 15.68\n",
      "\n",
      "------------------------------------resnet18------------------------------------\n",
      "PoseResNet\t\tmacs=8307625984.0\tparams=15381589.0\n",
      "FLOPs= 8.307625984G\tparams= 15.381589M\n",
      "macs='8.308G'\tparams='15.382M'\n",
      "Mean@ 49.663ms Std@ 16.047ms FPS@ 20.14\n",
      "\n",
      "------------------------------------resnet50------------------------------------\n",
      "PoseResNet\t\tmacs=12070785024.0\tparams=30615861.0\n",
      "FLOPs= 12.070785024G\tparams= 30.615861M\n",
      "macs='12.071G'\tparams='30.616M'\n",
      "Mean@ 101.026ms Std@ 25.895ms FPS@ 9.90\n",
      "\n",
      "----------------------------------mobilenetv2-----------------------------------\n",
      "PoseMobileNetV2\t\tmacs=7153088512.0\tparams=9587893.0\n",
      "FLOPs= 7.153088512G\tparams= 9.587893M\n",
      "macs='7.153G'\tparams='9.588M'\n",
      "Mean@ 61.452ms Std@ 20.344ms FPS@ 16.27\n",
      "\n",
      "----------------------------------litehrnet18-----------------------------------\n",
      "LiteHRNet\t\tmacs=422691444.0\tparams=1483873.0\n",
      "FLOPs= 0.422691444G\tparams= 1.483873M\n",
      "macs='422.691M'\tparams='1.484M'\n",
      "Mean@ 257.834ms Std@ 49.370ms FPS@ 3.88\n",
      "\n",
      "----------------------------------litehrnet30-----------------------------------\n",
      "LiteHRNet\t\tmacs=559187852.0\tparams=1773361.0\n",
      "FLOPs= 0.559187852G\tparams= 1.773361M\n",
      "macs='559.188M'\tparams='1.773M'\n",
      "Mean@ 336.796ms Std@ 57.237ms FPS@ 2.97\n",
      "\n",
      "----------------------------------hourglass_2-----------------------------------\n",
      "HourglassNet\t\tmacs=8421761024.0\tparams=6574250.0\n",
      "FLOPs= 8.421761024G\tparams= 6.57425M\n",
      "macs='8.422G'\tparams='6.574M'\n",
      "Mean@ 118.332ms Std@ 27.609ms FPS@ 8.45\n",
      "\n",
      "----------------------------------hourglass_1-----------------------------------\n",
      "HourglassNet\t\tmacs=5202833408.0\tparams=3427733.0\n",
      "FLOPs= 5.202833408G\tparams= 3.427733M\n",
      "macs='5.203G'\tparams='3.428M'\n",
      "Mean@ 73.651ms Std@ 24.200ms FPS@ 13.58\n",
      "\n",
      "-----------------------------------srhandnet------------------------------------\n",
      "SRHandNet\t\tmacs=12144135680.0\tparams=18255032.0\n",
      "FLOPs= 12.14413568G\tparams= 18.255032M\n",
      "macs='12.144G'\tparams='18.255M'\n",
      "Mean@ 94.988ms Std@ 25.201ms FPS@ 10.53\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(1, 3, 256, 256)\n",
    "for model_name, cfg in cfg_dict.items():\n",
    "    print(f\"{model_name}\".center(80, '-'))\n",
    "    cfg = addict.Dict(cfg)\n",
    "    model = eval(cfg.MODEL.name)(cfg)\n",
    "    # show_model_stat(model)\n",
    "    show_macs_params(model, dummy_input=x)\n",
    "    inference_speed(model, dummy_input=x)\n",
    "    print()\n",
    "    if model_name == 'litehandnet':\n",
    "        model.deploy_model()\n",
    "        show_macs_params(model, dummy_input=x)\n",
    "        inference_speed(model, dummy_input=x)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "观察消融实验的结果，可以看出MS_Att模块中的MSRB-DWConv运算速度非常慢"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------cfg_idx=0\t(224, 224)------------------------------\n",
      "hourglass_ablation\t\tmacs=4390493952.0\tparams=8894229.0\n",
      "FLOPs= 4.390493952G\tparams= 8.894229M\n",
      "macs='4.390G'\tparams='8.894M'\n",
      "Mean@ 100.644ms Std@ 25.156ms FPS@ 9.94\n",
      "\n",
      "------------------------------cfg_idx=1\t(224, 224)------------------------------\n",
      "hourglass_ablation\t\tmacs=4390493952.0\tparams=8894229.0\n",
      "FLOPs= 4.390493952G\tparams= 8.894229M\n",
      "macs='4.390G'\tparams='8.894M'\n",
      "Mean@ 98.995ms Std@ 22.644ms FPS@ 10.10\n",
      "\n",
      "------------------------------cfg_idx=2\t(224, 224)------------------------------\n",
      "hourglass_ablation\t\tmacs=4390493952.0\tparams=8894229.0\n",
      "FLOPs= 4.390493952G\tparams= 8.894229M\n",
      "macs='4.390G'\tparams='8.894M'\n",
      "Mean@ 96.701ms Std@ 24.197ms FPS@ 10.34\n",
      "\n",
      "------------------------------cfg_idx=3\t(224, 224)------------------------------\n",
      "hourglass_ablation\t\tmacs=4390493952.0\tparams=8894229.0\n",
      "FLOPs= 4.390493952G\tparams= 8.894229M\n",
      "macs='4.390G'\tparams='8.894M'\n",
      "Mean@ 99.339ms Std@ 23.206ms FPS@ 10.07\n",
      "\n",
      "------------------------------cfg_idx=0\t(256, 256)------------------------------\n",
      "hourglass_ablation\t\tmacs=4390493952.0\tparams=8894229.0\n",
      "FLOPs= 4.390493952G\tparams= 8.894229M\n",
      "macs='4.390G'\tparams='8.894M'\n",
      "Mean@ 99.634ms Std@ 23.431ms FPS@ 10.04\n",
      "\n",
      "------------------------------cfg_idx=1\t(256, 256)------------------------------\n",
      "hourglass_ablation\t\tmacs=4390493952.0\tparams=8894229.0\n",
      "FLOPs= 4.390493952G\tparams= 8.894229M\n",
      "macs='4.390G'\tparams='8.894M'\n",
      "Mean@ 98.028ms Std@ 23.114ms FPS@ 10.20\n",
      "\n",
      "------------------------------cfg_idx=2\t(256, 256)------------------------------\n",
      "hourglass_ablation\t\tmacs=4390493952.0\tparams=8894229.0\n",
      "FLOPs= 4.390493952G\tparams= 8.894229M\n",
      "macs='4.390G'\tparams='8.894M'\n",
      "Mean@ 98.535ms Std@ 23.440ms FPS@ 10.15\n",
      "\n",
      "------------------------------cfg_idx=3\t(256, 256)------------------------------\n",
      "hourglass_ablation\t\tmacs=4390493952.0\tparams=8894229.0\n",
      "FLOPs= 4.390493952G\tparams= 8.894229M\n",
      "macs='4.390G'\tparams='8.894M'\n"
     ]
    }
   ],
   "source": [
    "from models import hourglass_ablation\n",
    "cfgs = {\n",
    "    0:dict(MODEL=dict(\n",
    "            name='hourglass_ablation',\n",
    "            input_channel=256,\n",
    "            output_channel=21,       # num_joints + 3 region map\n",
    "            num_stack=2,            # 沙漏模块的个数\n",
    "            num_level=4,            # 每个沙漏模块不同尺度特征层个数\n",
    "            pelee_stem=dict(enabled=False, focus=False),\n",
    "            msrb_att=dict(enabled=False, att_enabled=False),\n",
    "            pred_bbox=True,          # 模型是否预测边界框, 是则不进行旋转变换\n",
    "            )),\n",
    "    1:dict(MODEL=dict(\n",
    "            name='hourglass_ablation',\n",
    "            input_channel=256,\n",
    "            output_channel=21,       # num_joints + 3 region map\n",
    "            num_stack=2,            # 沙漏模块的个数\n",
    "            num_level=4,            # 每个沙漏模块不同尺度特征层个数\n",
    "            pelee_stem=dict(enabled=True, focus=False),\n",
    "            msrb_att=dict(enabled=False, att_enabled=False),\n",
    "            pred_bbox=True,          # 模型是否预测边界框, 是则不进行旋转变换\n",
    "            )),\n",
    "    2:dict(MODEL=dict(\n",
    "            name='hourglass_ablation',\n",
    "            input_channel=256,\n",
    "            output_channel=21,       # num_joints + 3 region map\n",
    "            num_stack=2,            # 沙漏模块的个数\n",
    "            num_level=4,            # 每个沙漏模块不同尺度特征层个数\n",
    "            pelee_stem=dict(enabled=False, focus=False),\n",
    "            msrb_att=dict(enabled=True, att_enabled=False),\n",
    "            pred_bbox=True,          # 模型是否预测边界框, 是则不进行旋转变换\n",
    "            )),\n",
    "    3:dict(MODEL=dict(\n",
    "            name='hourglass_ablation',\n",
    "            input_channel=256,\n",
    "            output_channel=21,       # num_joints + 3 region map\n",
    "            num_stack=2,            # 沙漏模块的个数\n",
    "            num_level=4,            # 每个沙漏模块不同尺度特征层个数\n",
    "            pelee_stem=dict(enabled=False, focus=False),\n",
    "            msrb_att=dict(enabled=True, att_enabled=True),\n",
    "            pred_bbox=True,          # 模型是否预测边界框, 是则不进行旋转变换\n",
    "            )),\n",
    "}\n",
    "for img_size in [(224, 224), (256, 256)]:\n",
    "    for cfg_idx in range(len(cfgs)):\n",
    "        print(f\"{cfg_idx=}\\t{img_size}\".center(80, '-'))\n",
    "        cfg = addict.Dict(cfgs[cfg_idx]) \n",
    "        model = hourglass_ablation(cfg)\n",
    "        show_macs_params(model, img_size=(256, 256))\n",
    "        inference_speed(model, img_size=(256, 256))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from turtle import forward\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from models import kaiming_init, constant_init, normal_init\n",
    "\n",
    "\n",
    "class CBL(nn.Module):\n",
    "    def __init__(self, in_channel, out_channel, kernel=1,\n",
    "                 stride=1, padding=0, dilation=1, groups=1):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channel, out_channel, kernel, stride,\n",
    "                      padding, dilation, groups, bias=False),\n",
    "            nn.BatchNorm2d(out_channel),\n",
    "            nn.LeakyReLU(inplace=True)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class ChannelAttension(nn.Module):\n",
    "    def __init__(self, channel):\n",
    "        super().__init__()\n",
    "        self.att = nn.Sequential(\n",
    "                    nn.AdaptiveAvgPool2d((3,3)),\n",
    "                    nn.BatchNorm2d(channel),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Conv2d(channel, channel, 3, 1, 0, groups=channel),\n",
    "                    nn.Flatten(),\n",
    "                    nn.Dropout(p=0.3),\n",
    "                    nn.Linear(channel, channel),\n",
    "                    nn.Sigmoid(),  \n",
    "                    )\n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.shape\n",
    "        x = x * self.att(x).view(b, c, 1, 1)\n",
    "        return x\n",
    "\n",
    "\n",
    "class MSRB(nn.Module):\n",
    "    def __init__(self, channel):\n",
    "        super().__init__()\n",
    "        c1 = channel // 2\n",
    "        c2 = channel * 2\n",
    "        self.conv11 = CBL(c1, c1, 3, 1, 1, 1, groups=c1)\n",
    "        self.conv12 = CBL(c1, c1, 3, 1, 2, 2, groups=c1)\n",
    "        self.conv13 = CBL(channel, channel)   # 信息融合，交换\n",
    "        self.conv21 = CBL(channel, channel, 3, 1, 1, 1, groups=channel)\n",
    "        self.conv22 = CBL(channel, channel, 3, 1, 2, 2, groups=channel)\n",
    "        # 信息融合，交换， 变通道\n",
    "        self.conv23 = nn.Conv2d(c2, channel, 1, 1, 0, bias=False)   \n",
    "\n",
    "        self.ca = ChannelAttension(channel)\n",
    "    def forward(self, x):\n",
    "        x1, x2 = torch.chunk(x, 2, dim=1)\n",
    "        a1 = self.conv11(x1)\n",
    "        a2 = self.conv12(x2)\n",
    "        a = torch.cat([a1, a2], dim=1)\n",
    "        a = self.conv13(a)\n",
    "        b1 = self.conv21(a)\n",
    "        b2 = self.conv22(a)\n",
    "        b = torch.cat([b1, b2], dim=1)\n",
    "        b = self.conv23(b)\n",
    "        out =  b + x  # skip conection\n",
    "        out = self.ca(out)\n",
    "        return out\n",
    "\n",
    "class CSP_MSRB(nn.Module):\n",
    "    def __init__(self, in_channel, out_channel):\n",
    "        super().__init__()\n",
    "        assert in_channel % 2 == 0\n",
    "        mid_channel = in_channel // 2\n",
    "        self.conv1 = CBL(in_channel, mid_channel)\n",
    "        self.msrb = MSRB(mid_channel)\n",
    "        self.conv2 = nn.Conv2d(mid_channel, mid_channel, 1, 1, 0)\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.BatchNorm2d(in_channel),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            CBL(in_channel, out_channel)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x1 = self.msrb(x)\n",
    "        x2 = self.conv2(x)\n",
    "        out = self.conv3(torch.cat([x1, x2], dim=1))\n",
    "        return out\n",
    "\n",
    "\n",
    "class EncoderDecoder(nn.Module):\n",
    "    def __init__(self, num_stage=4, num_block=[2,2,2,2], in_channel=128):\n",
    "        super().__init__()\n",
    "        self.num_levels = num_stage\n",
    "        self.encoder = nn.ModuleList([])\n",
    "        self.decoder = nn.ModuleList([])\n",
    "\n",
    "        self.maxpool = nn.MaxPool2d(2, 2)\n",
    "        for i in range(num_stage):\n",
    "            nb = num_block[i]\n",
    "            self.encoder.append(\n",
    "                nn.Sequential(*[CSP_MSRB(in_channel, in_channel) for _ in range(nb)])\n",
    "            )\n",
    "            self.decoder.append(\n",
    "                 nn.Sequential(*[CSP_MSRB(in_channel, in_channel) for _ in range(nb)])\n",
    "            )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        out_encoder = []   # [128, 64, 32, 16, 8, 4]\n",
    "        out_decoder = []   # [4, 8, 16, 32, 64, 128]\n",
    "\n",
    "        # encoder \n",
    "        for i in range(self.num_levels):\n",
    "            x = self.encoder[i](x)\n",
    "            out_encoder.append(x)\n",
    "            if i != self.num_levels - 1:\n",
    "                x = self.maxpool(x)\n",
    "\n",
    "        # decoder\n",
    "        for i in range(self.num_levels-1, -1, -1):\n",
    "            counterpart = out_encoder[i]\n",
    "            if i == self.num_levels-1:\n",
    "                x = self.decoder[i](counterpart)\n",
    "            else:\n",
    "                h, w = counterpart.shape[2:]\n",
    "                x = F.interpolate(x, size=(h, w))\n",
    "                x = x + counterpart\n",
    "                x = self.decoder[i](x)\n",
    "            out_decoder.append(x)\n",
    "        return tuple(out_decoder) \n",
    "\n",
    "class Stem(nn.Module):\n",
    "    def __init__(self, channel):\n",
    "        super().__init__()\n",
    "        mid_channel = max(channel // 4, 32)\n",
    "        self.conv1 = nn.Sequential(\n",
    "            CBL(3, mid_channel, 3, 2, 1),\n",
    "            CBL(mid_channel, mid_channel, 3, 1, 1, groups=mid_channel)\n",
    "        )\n",
    "        self.branch1 = nn.Sequential(\n",
    "            CBL(mid_channel, mid_channel),\n",
    "            CBL(mid_channel, mid_channel, 3, 2, 1)\n",
    "        )\n",
    "        self.branch2 = nn.MaxPool2d(2, 2, ceil_mode=True)\n",
    "        self.conv2 = nn.Conv2d(2*mid_channel, channel, 1, 1, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        b1 = self.branch1(out)\n",
    "        b2 = self.branch1(out)\n",
    "        out = self.conv2(torch.cat([b1, b2], dim=1))\n",
    "        return out\n",
    "\n",
    "class LiteHandNet(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        num_stage=cfg.MODEL.get('num_stage', 4)\n",
    "        num_block=cfg.MODEL.get('num_block', [2, 2, 2, 2])\n",
    "        input_channel=cfg.MODEL.get('input_channel', 128)\n",
    "        output_channel=cfg.MODEL.get('output_channel', cfg.DATASET.num_joints)\n",
    "\n",
    "        self.stem = Stem(input_channel)\n",
    "        self.backone = EncoderDecoder(num_stage, num_block, input_channel)\n",
    "        self.neck = CBL(input_channel, input_channel)\n",
    "        self.head = nn.Conv2d(input_channel, output_channel, 1, 1, 0)\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out_stem = self.stem(x)\n",
    "        out_backbone = self.backone(out_stem)\n",
    "\n",
    "        out_backbone_last_stage = out_backbone[-1]\n",
    "        out_neck = self.neck(out_backbone_last_stage)\n",
    "        out = self.head(out_neck)\n",
    "        return out\n",
    "\n",
    "    def init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                # kaiming_init(m)\n",
    "                normal_init(m)\n",
    "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
    "                constant_init(m, 1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import addict\n",
    "cfg = dict(MODEL=dict(\n",
    "    num_stage=4,\n",
    "    num_block=[1, 4, 6, 6],\n",
    "    increase=False,\n",
    "    input_channel=128,\n",
    "    output_channel=21,\n",
    "))\n",
    "cfg = addict.Dict(cfg)\n",
    "\n",
    "model = LiteHandNet(cfg)\n",
    "for img_size in [(224, 224), (256, 256)]:\n",
    "        print(f\"{img_size}\".center(80, '-'))\n",
    "        show_macs_params(model, img_size=(256, 256))\n",
    "        inference_speed(model, img_size=(256, 256))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3、统计各个数据集的图片个数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "freihand_train.json => 104192\n",
      "freihand_train.json => 104192\n",
      "freihand100.json => 100\n",
      "freihand100.json => 100\n",
      "freihand_val.json => 13024\n",
      "freihand_val.json => 13024\n",
      "freihand_test.json => 13024\n",
      "freihand_test.json => 13024\n",
      "freihand100_train.json => 100\n",
      "freihand100_train.json => 100\n",
      "freihand100_val.json => 100\n",
      "freihand100_val.json => 100\n",
      "freihand_train_6400.json => 6400\n",
      "freihand_train_6400.json => 6400\n",
      "freihand_val_6400.json => 6400\n",
      "freihand_val_6400.json => 6400\n",
      "\n",
      "onehand10k_test.json => 1703\n",
      "onehand10k_test.json => 1703\n",
      "onehand10k_train.json => 10000\n",
      "onehand10k_train.json => 10000\n",
      "\n",
      "panoptic_test.json => 846\n",
      "panoptic_test.json => 846\n",
      "panoptic_train.json => 16729\n",
      "panoptic_train.json => 16729\n",
      "\n",
      "rhd_test.json => 2727\n",
      "rhd_test.json => 2727\n",
      "rhd_train.json => 41255\n",
      "rhd_train.json => 41255\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "freihand_ann_root = \"data/handset/freihand/annotations\"\n",
    "rhd_ann_root = \"data/handset/OneHand10K/annotations\"\n",
    "panoptic_ann_root = \"data/handset/panoptic/annotations\"\n",
    "onehand10k_ann_root = \"data/handset/RHD/annotations\"\n",
    "\n",
    "for ann_root in [freihand_ann_root, rhd_ann_root, panoptic_ann_root, onehand10k_ann_root]:\n",
    "    files = os.listdir(ann_root)\n",
    "    ann_root = Path(ann_root)\n",
    "    for file in files:\n",
    "        ann_file = ann_root.joinpath(file)\n",
    "        json_file = json.load(ann_file.open(mode='r'))\n",
    "        print(f\"{file} => {len(json_file['images'])}\")\n",
    "        print(f\"{file} => {len(json_file['annotations'])}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y.shape=torch.Size([1, 21, 2])\n",
      "light_Model\t\tmacs=2253977686.0\tparams=1894628.0\n",
      "FLOPs= 2.253977686G\tparams= 1.894628M\n",
      "macs='2.254G'\tparams='1.895M'\n",
      "Mean@ 233.892ms Std@ 3.600ms FPS@ 4.28\n"
     ]
    }
   ],
   "source": [
    "from models import atthandnet\n",
    "model = atthandnet(None)\n",
    "model.eval()\n",
    "x = torch.rand(1, 3, 224, 224)\n",
    "y = model(x)\n",
    "print(f\"{y.shape=}\")\n",
    "show_macs_params(model, dummy_input=x)\n",
    "inference_speed(model, dummy_input=x, repetitions=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiScaleAttentionHourglass\t\tmacs=1750821216.0\tparams=3490101.0\n",
      "FLOPs= 1.750821216G\tparams= 3.490101M\n",
      "macs='1.751G'\tparams='3.490M'\n",
      "Mean@ 17.776ms Std@ 0.479ms FPS@ 56.25\n"
     ]
    }
   ],
   "source": [
    "from models import mynet\n",
    "cfg=dict(MODEL=dict(name='mynet',\n",
    "                        input_channel=160,\n",
    "                        output_channel=21,       # num_joints + 3 region map\n",
    "                        num_stage=4,\n",
    "                        num_block=[2, 2, 2],\n",
    "                        ))\n",
    "model = mynet(addict.Dict(cfg))\n",
    "x = torch.rand(1, 3, 256, 256)\n",
    "show_macs_params(model, dummy_input=x)\n",
    "inference_speed(model, dummy_input=x, repetitions=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 65536])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand(1, 3, 256, 256)\n",
    "y = x.view(3, -1)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torch' has no attribute 'view'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_10969/3754981264.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: module 'torch' has no attribute 'view'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (256) must match the size of tensor b (65536) at non-singleton dimension 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_10969/2055811296.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (256) must match the size of tensor b (65536) at non-singleton dimension 3"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "435570d4219e70938454f0c8f629267d4bfa46e86b2ba3c4b1d73b5202317604"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('TorchCV')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
